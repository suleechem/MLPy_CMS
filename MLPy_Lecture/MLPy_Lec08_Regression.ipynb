{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9a951e1",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/8-1.png\" style=\"max-width: 60%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e066fe13",
   "metadata": {},
   "source": [
    "- 입력(x)과 출력(y)은 <font color=red> $y=Wx+b$ </font> 형태로 나타낼 수 있음.\n",
    "- 1,2,3 $\\dots$ 등의 <font color=red>다양한 $y=Wx+b$ 직선 가능</font>\n",
    "- training data의 특성을 가장 잘 표시 할 수 있는 <font color=red>가중치(weight) $W$(기울기), 바이어스(bias) $b$(y 절편)</font>을 찾는 것이 학습(Learning)개념. \n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/8-2.png\" style=\"max-width: 40%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7860cb8",
   "metadata": {},
   "source": [
    "###### - Training data의 정답$t$과 직선 $y=Wx+b$ 값의 차이인 오차(error)는    \n",
    "<font color=red>$ error = t - y = t - (Wx+b)$</font>\n",
    "- Linear regression은 모든 데이터의 오차$(error) = t - y = t - (Wx+b)$의 합이 최소가 되도록 가중치 $W$와 바이어스 $b$값을 결정함.\n",
    "\n",
    "- <font color=red><b>손실함수(loss function 또는 cost function)</b></font>는 training data의 정답(t)과 계산값(y)의 차이를 모두 합한 수식</font>\n",
    "- 각각의 오차인 $t-y$ 값이 $(+),(-)$ 모두 가능하기 때문에, 오차의 합이 $0$이 나올 수 있음.\n",
    "- 따라서,<font color=red> $y=Wx+b \\ , \\ loss function$으로는 최소 오차를 판단하기 어려움.</font>\n",
    "\n",
    "- <font color=blue><b>loss function에서 $(error) = (t - y)^2 = (t - (Wx+b))^2$</b></font>를 사용함.\n",
    "- 오차는 항상 양수이며, 정답과의 차이가 크면 제곱에 의해 오차는 더 커짐.\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/8-3.png\" style=\"max-width: 60%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a99a71b",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "$ loss \\ function $  \n",
    "\n",
    "$ \\displaystyle\n",
    "=\\frac\n",
    "{(t_1-y_1)^2 + (t_1-y_1)^2 + \\dots + (t_n-y_n)^2}\n",
    "{n}\n",
    "$\n",
    "    \n",
    "$ \\displaystyle\n",
    "=\\frac\n",
    "{[t_1-(Wx_1+b)]^2 + [t_2-(Wx_2+b)]^2 + \\dots + [t_n-(Wx_n+b)]^2}\n",
    "{n}\n",
    "$\n",
    "\n",
    "$ \\displaystyle\n",
    "=\\frac{1}{n}\n",
    "\\sum^{n}_{i=1}[t_i-(Wx_i + b)]^2\n",
    "$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1594f01a",
   "metadata": {},
   "source": [
    "- 손실함수 $ \\displaystyle loss \\ function = E(W,b) $\n",
    "- <font color=red> $loss \\ function = E(W,b)$가 최소값을 갖도록 $W,b$를 구하는 것이 $(linear) \\ regression$의 최종 목적임.</font> \n",
    "\n",
    "<br>\n",
    "<font size=3>\n",
    "$ \\displaystyle\n",
    "y=Wx+b\n",
    "$\n",
    "    \n",
    "$ \\displaystyle loss \\ function \n",
    "= E(W,b) \n",
    "=\\frac{1}{n}\n",
    "\\sum^{n}_{i=1}[t_i-(Wx_i + b)]^2\n",
    "$\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d979e",
   "metadata": {},
   "source": [
    "# Gradient decent algorithm (경사하강법)\n",
    "\n",
    "- 손실함수 (loss function) 계산\n",
    "- $E(W,b)$ 에서 $bias \\ b=0$ 으로 가정\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/8-4.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ea18b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   W   E(W)\n",
      "0 -1  18.70\n",
      "1  0   4.67\n",
      "2  1   0.00\n",
      "3  2   4.67\n",
      "4  3  18.70\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "data = {\n",
    "    'W': [-1,0,1,2,3],\n",
    "    'E(W)': [18.7,4.67,0,4.67,18.7],\n",
    "}\n",
    " \n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "plt.style.use('default')\n",
    "plt.rcParams['figure.figsize'] = (4, 4)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "\n",
    "x = np.linspace(-1, 3, 1000)\n",
    "y = 4.67*(x-1)**2\n",
    "x_data = df['W']\n",
    "y_data = df['E(W)']\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.plot(x_data, y_data, 'bo')\n",
    "\n",
    "plt.title('E(W)')\n",
    "plt.grid(color='0.8')\n",
    "ax.spines['left'].set_position(('data',0)) \n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['top'].set_visible(False) \n",
    "ax.spines['bottom'].set_position(('data',0)) \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613d56d1",
   "metadata": {},
   "source": [
    "# Gradient decent algorithm (경사하강법) - $W$ 값 구하기\n",
    "\n",
    "- $ \\displaystyle \\frac{\\partial{E(W)}}{\\partial{W}}$ 는 특정 $W$에서의 $E(W)$의 기울기$slope$ 을 의미\n",
    "\n",
    "\n",
    "- <font color=red> $ \\displaystyle \\frac{\\partial{E(W)}}{\\partial{W}}$ 이 양수 $(+)$값</font>을 갖는다면, <font color=red>$W$는 왼쪽</font>으로 이동시켜야 $E(W)$가 감소\n",
    "\n",
    "\n",
    "- <font color=red> $ \\displaystyle \\frac{\\partial{E(W)}}{\\partial{W}}$ 이 음수 $(-)$값</font>을 갖는다면, <font color=red>$W$는 오른쪽</font>으로 이동시켜야 $E(W)$가 감소\n",
    "\n",
    "<font size=3><b>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$ \\displaystyle W = W - \\alpha \\frac{\\partial{E(W,b)}}{\\partial{W}}$\n",
    "</b></font>\n",
    "\n",
    "\n",
    "- <font color=red> $\\alpha$는 학습율$(learning \\ rate)$</font>이라고 하며, $W$값이 감소 또는 증가하는 비율을 나타냄.\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/8-6.png\" style=\"max-width: 50%; height: auto;\">\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/8-5.png\" style=\"max-width: 60%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bdc1824",
   "metadata": {},
   "source": [
    "<font size=3>\n",
    "$ \\displaystyle\n",
    "y=Wx+b\n",
    "$\n",
    "    \n",
    "$ \\displaystyle  \n",
    "E(W,b) \n",
    "=\\frac{1}{n}\n",
    "\\sum^{n}_{i=1}[t_i-(Wx_i + b)]^2\n",
    "$\n",
    "\n",
    "<font color=blue> 손실함수 $E(W,b)$를 최소화하는 $W \\ and \\ b$   </font>\n",
    "\n",
    "<br>\n",
    "<font size=3 color=blue><b>\n",
    "$ \\displaystyle W = W - \\alpha \\frac{\\partial{E(W,b)}}{\\partial{W}} \\ \\ \\ \\ , \\ \\ \\ \\ \n",
    "     b = b - \\alpha \\frac{\\partial{E(W,b)}}{\\partial{b}}$\n",
    "</b></font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4073256a",
   "metadata": {},
   "source": [
    "# Linear regression using python\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/8-7.png\" style=\"max-width: 60%; height: auto;\">\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/8-8.png\" style=\"max-width: 60%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e70c17",
   "metadata": {},
   "source": [
    "## 1. Training Data 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b79c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "t_data = np.array([2,3,4,5,6]).reshape(5,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4d2ac8",
   "metadata": {},
   "source": [
    "## 2. 임의의 직선 $y=Wx+b$ 정의 (임의의 $W, \\ b$ 초기화)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fac77609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W=[[0.8256598]], W.shape=(1, 1), b=[0.68908592], b.shape=(1,)\n"
     ]
    }
   ],
   "source": [
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "print(f\"W={W}, W.shape={W.shape}, b={b}, b.shape={b.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3652d85",
   "metadata": {},
   "source": [
    "## 3. 손실함수 $E(W)$ 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc63c3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_func(x,t):\n",
    "    y=np.dot(x,W)+b\n",
    "    return (np.sum((t-y)**2)/(len(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c80db63",
   "metadata": {},
   "source": [
    "## 4. 수치미분 numerical_derivative 및 utility 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0b807a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_derivative(f,x):  \n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)  \n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx=it.multi_index\n",
    "        tmp_val=x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) \n",
    "        \n",
    "        x[idx] = float(tmp_val) - delta_x\n",
    "        fx2 = f(x)  \n",
    "        grad[idx] = (fx1 - fx2)/(2*delta_x)\n",
    "\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def error_val(x,t):\n",
    "    y=np.dot(x,W)+b\n",
    "    return (np.sum((t-y)**2))/(len(x))\n",
    "\n",
    "def predict(x):\n",
    "    y=np.dot(x,W)+b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bc396e",
   "metadata": {},
   "source": [
    "## 5. 학습율(learning rate) 초기화 및 $W, b$ 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b512ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial error value = 0.7562360761676125, initial W = [[0.8256598]]\n",
      "step = 0, error_value = 0.44957478257989336, W = [[0.88266949]], b = [0.70234403]\n",
      "step = 400, error_value = 0.0007666217775277836, W = [[1.0179799]], b = [0.93510274]\n",
      "step = 800, error_value = 4.891500165338907e-05, W = [[1.00454169]], b = [0.98360706]\n",
      "step = 1200, error_value = 3.1210662896461846e-06, W = [[1.00114722]], b = [0.99585917]\n",
      "step = 1600, error_value = 1.991424809386296e-07, W = [[1.00028979]], b = [0.99895403]\n",
      "step = 2000, error_value = 1.270646760868696e-08, W = [[1.0000732]], b = [0.99973579]\n",
      "step = 2400, error_value = 8.107477537488164e-10, W = [[1.00001849]], b = [0.99993326]\n",
      "step = 2800, error_value = 5.1730499810987217e-11, W = [[1.00000467]], b = [0.99998314]\n",
      "step = 3200, error_value = 3.300711718042834e-12, W = [[1.00000118]], b = [0.99999574]\n",
      "step = 3600, error_value = 2.1060492119993605e-13, W = [[1.0000003]], b = [0.99999892]\n",
      "step = 4000, error_value = 1.3437839147147941e-14, W = [[1.00000008]], b = [0.99999973]\n",
      "step = 4400, error_value = 8.574135803052226e-16, W = [[1.00000002]], b = [0.99999993]\n",
      "step = 4800, error_value = 5.470805637181197e-17, W = [[1.]], b = [0.99999998]\n",
      "step = 5200, error_value = 3.490696905697148e-18, W = [[1.]], b = [1.]\n",
      "step = 5600, error_value = 2.227274510398741e-19, W = [[1.]], b = [1.]\n",
      "step = 6000, error_value = 1.4211368593093828e-20, W = [[1.]], b = [1.]\n",
      "step = 6400, error_value = 9.067640896419327e-22, W = [[1.]], b = [1.]\n",
      "step = 6800, error_value = 5.785268234543203e-23, W = [[1.]], b = [1.]\n",
      "step = 7200, error_value = 3.6929324998206634e-24, W = [[1.]], b = [1.]\n",
      "step = 7600, error_value = 2.36339344963084e-25, W = [[1.]], b = [1.]\n",
      "step = 8000, error_value = 1.510834294288334e-26, W = [[1.]], b = [1.]\n"
     ]
    }
   ],
   "source": [
    "learning_rate=1e-2\n",
    "\n",
    "f = lambda x: loss_func(x_data,t_data)  # f(x) = loass_func(x_data,y_data)\n",
    "\n",
    "print(f\"initial error value = {error_val(x_data,t_data)}, initial W = {W}\")\n",
    "\n",
    "for step in range(8001):\n",
    "    W-= learning_rate*numerical_derivative(f,W)\n",
    "    b-= learning_rate*numerical_derivative(f,b)\n",
    "    \n",
    "    if(step % 400 ==0):\n",
    "        print(f\"step = {step}, error_value = {error_val(x_data,t_data)}, W = {W}, b = {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "698637c2",
   "metadata": {},
   "source": [
    "## 6. Linear Regression Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d14598c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W=[[0.39811734]], W.shape=(1, 1), b=[0.70666491], b.shape=(1,)\n",
      "initial error value = 5.130255385258174, initial W = [[0.39811734]]\n",
      "step = 0, error_value = 3.019701265030401, W = [[0.54813163]], b = [0.73964371]\n",
      "step = 400, error_value = 0.0002737144523357534, W = [[1.0107435]], b = [0.96122207]\n",
      "step = 800, error_value = 1.7464600251426445e-05, W = [[1.00271379]], b = [0.99020476]\n",
      "step = 1200, error_value = 1.114344746282679e-06, W = [[1.0006855]], b = [0.99752574]\n",
      "step = 1600, error_value = 7.110178278883542e-08, W = [[1.00017316]], b = [0.99937501]\n",
      "step = 2000, error_value = 4.536714093713078e-09, W = [[1.00004374]], b = [0.99984213]\n",
      "step = 2400, error_value = 2.894691801153872e-10, W = [[1.00001105]], b = [0.99996012]\n",
      "step = 2800, error_value = 1.846984502495745e-11, W = [[1.00000279]], b = [0.99998993]\n",
      "step = 3200, error_value = 1.1784853055207133e-12, W = [[1.0000007]], b = [0.99999746]\n",
      "step = 3600, error_value = 7.519432994604196e-14, W = [[1.00000018]], b = [0.99999936]\n",
      "step = 4000, error_value = 4.797842721177034e-15, W = [[1.00000004]], b = [0.99999984]\n",
      "step = 4400, error_value = 3.0613074135272874e-16, W = [[1.00000001]], b = [0.99999996]\n",
      "step = 4800, error_value = 1.9532951658388128e-17, W = [[1.]], b = [0.99999999]\n",
      "step = 5200, error_value = 1.2463181347923895e-18, W = [[1.]], b = [1.]\n",
      "step = 5600, error_value = 7.952260574396494e-20, W = [[1.]], b = [1.]\n",
      "step = 6000, error_value = 5.074012076754006e-21, W = [[1.]], b = [1.]\n",
      "step = 6400, error_value = 3.2375899884290565e-22, W = [[1.]], b = [1.]\n",
      "step = 6800, error_value = 2.065791413287325e-23, W = [[1.]], b = [1.]\n",
      "step = 7200, error_value = 1.3199435729362255e-24, W = [[1.]], b = [1.]\n",
      "step = 7600, error_value = 8.466332322224857e-26, W = [[1.]], b = [1.]\n",
      "step = 8000, error_value = 5.454065969562293e-27, W = [[1.]], b = [1.]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[44.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.array([1,2,3,4,5]).reshape(5,1)\n",
    "t_data = np.array([2,3,4,5,6]).reshape(5,1)\n",
    "\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "print(f\"W={W}, W.shape={W.shape}, b={b}, b.shape={b.shape}\")\n",
    "\n",
    "def loss_func(x,t):\n",
    "    y=np.dot(x,W)+b\n",
    "    return (np.sum((t-y)**2)/(len(x)))\n",
    "\n",
    "def numerical_derivative(f,x):  \n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)  \n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx=it.multi_index\n",
    "        tmp_val=x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) \n",
    "        \n",
    "        x[idx] = float(tmp_val) - delta_x\n",
    "        fx2 = f(x)  \n",
    "        grad[idx] = (fx1 - fx2)/(2*delta_x)\n",
    "\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def error_val(x,t):\n",
    "    y=np.dot(x,W)+b\n",
    "    return (np.sum((t-y)**2)/(len(x)))\n",
    "            \n",
    "def predict(x):\n",
    "    y=np.dot(x,W)+b\n",
    "    return y\n",
    "\n",
    "learning_rate=1e-2\n",
    "\n",
    "f = lambda x: loss_func(x_data,t_data)  # f(x) = loass_func(x_data,y_data)\n",
    "\n",
    "print(f\"initial error value = {error_val(x_data,t_data)}, initial W = {W}\")\n",
    "\n",
    "for step in range(8001):\n",
    "    W -= learning_rate*numerical_derivative(f,W)\n",
    "    b -= learning_rate*numerical_derivative(f,b)\n",
    "    \n",
    "    if(step % 400 ==0):\n",
    "        print(f\"step = {step}, error_value = {error_val(x_data,t_data)}, W = {W}, b = {b}\")\n",
    "        \n",
    "predict(43)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0955c",
   "metadata": {},
   "source": [
    "## 7. Multi-variable Linear Regression Code\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/8-9.png\" style=\"max-width: 60%; height: auto;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91f02b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W=[[0.49810543]\n",
      " [0.18676845]\n",
      " [0.96711368]], W.shape=(3, 1), b=[0.46996304], b.shape=(1,)\n",
      "initial error value = 844.9252523591153, initial W = [[0.49810543]\n",
      " [0.18676845]\n",
      " [0.96711368]]\n",
      "step = 0, error_value = 317.32009982277776, W = [[0.54427499]\n",
      " [0.23332852]\n",
      " [1.01467774]], b = [0.47030963]\n",
      "step = 400, error_value = 7.474847132627126, W = [[0.59160902]\n",
      " [0.33235413]\n",
      " [1.08590656]], b = [0.47016018]\n",
      "step = 800, error_value = 7.214340597816979, W = [[0.56948084]\n",
      " [0.3557807 ]\n",
      " [1.0845277 ]], b = [0.4694718]\n",
      "step = 1200, error_value = 7.0089048715743045, W = [[0.54941127]\n",
      " [0.37621057]\n",
      " [1.08407656]], b = [0.46878374]\n",
      "step = 1600, error_value = 6.846198702947882, W = [[0.53121202]\n",
      " [0.39405401]\n",
      " [1.08433522]], b = [0.46809513]\n",
      "step = 2000, error_value = 6.716824233100628, W = [[0.51471148]\n",
      " [0.40966185]\n",
      " [1.08512763]], b = [0.4674053]\n",
      "step = 2400, error_value = 6.613581981622946, W = [[0.49975335]\n",
      " [0.42333449]\n",
      " [1.08631214]], b = [0.46671378]\n",
      "step = 2800, error_value = 6.530924187020552, W = [[0.48619534]\n",
      " [0.43532946]\n",
      " [1.0877754 ]], b = [0.46602025]\n",
      "step = 3200, error_value = 6.46455159547548, W = [[0.47390795]\n",
      " [0.44586781]\n",
      " [1.08942724]], b = [0.46532449]\n",
      "step = 3600, error_value = 6.411114417203391, W = [[0.46277343]\n",
      " [0.45513956]\n",
      " [1.09119641]], b = [0.46462636]\n",
      "step = 4000, error_value = 6.367989264670016, W = [[0.45268469]\n",
      " [0.46330827]\n",
      " [1.09302715]], b = [0.46392582]\n",
      "step = 4400, error_value = 6.3331117810015005, W = [[0.44354445]\n",
      " [0.47051492]\n",
      " [1.09487626]], b = [0.46322285]\n",
      "step = 4800, error_value = 6.304850296452563, W = [[0.43526428]\n",
      " [0.4768812 ]\n",
      " [1.09671073]], b = [0.46251749]\n",
      "step = 5200, error_value = 6.28190987578331, W = [[0.42776391]\n",
      " [0.48251232]\n",
      " [1.0985058 ]], b = [0.46180981]\n",
      "step = 5600, error_value = 6.263259005529778, W = [[0.42097042]\n",
      " [0.48749934]\n",
      " [1.1002433 ]], b = [0.46109989]\n",
      "step = 6000, error_value = 6.2480732463668325, W = [[0.41481765]\n",
      " [0.49192125]\n",
      " [1.10191038]], b = [0.46038783]\n",
      "step = 6400, error_value = 6.235691674706693, W = [[0.40924554]\n",
      " [0.49584662]\n",
      " [1.10349838]], b = [0.45967376]\n",
      "step = 6800, error_value = 6.225583024131533, W = [[0.40419961]\n",
      " [0.49933507]\n",
      " [1.105002  ]], b = [0.45895778]\n",
      "step = 7200, error_value = 6.217319228133045, W = [[0.39963043]\n",
      " [0.50243856]\n",
      " [1.10641849]], b = [0.45824002]\n",
      "step = 7600, error_value = 6.2105546440387975, W = [[0.39549317]\n",
      " [0.50520239]\n",
      " [1.10774715]], b = [0.45752059]\n",
      "step = 8000, error_value = 6.205009663135985, W = [[0.3917472 ]\n",
      " [0.50766615]\n",
      " [1.10898879]], b = [0.45679963]\n",
      "step = 8400, error_value = 6.20045772614602, W = [[0.38835565]\n",
      " [0.50986446]\n",
      " [1.11014538]], b = [0.45607724]\n",
      "step = 8800, error_value = 6.196714996629665, W = [[0.38528512]\n",
      " [0.51182768]\n",
      " [1.11121974]], b = [0.45535353]\n",
      "step = 9200, error_value = 6.193632119359995, W = [[0.38250534]\n",
      " [0.51358244]\n",
      " [1.11221527]], b = [0.45462862]\n",
      "step = 9600, error_value = 6.1910876218644155, W = [[0.37998886]\n",
      " [0.51515213]\n",
      " [1.11313576]], b = [0.4539026]\n",
      "step = 10000, error_value = 6.188982616550729, W = [[0.37771082]\n",
      " [0.51655735]\n",
      " [1.11398526]], b = [0.45317557]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([179.07968417])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "loaded_data = np.loadtxt('./data08.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "x_data = loaded_data[:, 0:-1]\n",
    "t_data = loaded_data[:,[-1]]\n",
    "\n",
    "# print(x_data, x_data.shape)\n",
    "# print(y_data, y_data.shape)\n",
    "\n",
    "W = np.random.rand(3,1)\n",
    "b = np.random.rand(1)\n",
    "print(f\"W={W}, W.shape={W.shape}, b={b}, b.shape={b.shape}\")\n",
    "\n",
    "def loss_func(x,t):\n",
    "    y=np.dot(x,W)+b\n",
    "    return (np.sum((t-y)**2)/(len(x)))\n",
    "\n",
    "def numerical_derivative(f,x):  \n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)  \n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx=it.multi_index\n",
    "        tmp_val=x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) \n",
    "        \n",
    "        x[idx] = float(tmp_val) - delta_x\n",
    "        fx2 = f(x)  \n",
    "        grad[idx] = (fx1 - fx2)/(2*delta_x)\n",
    "\n",
    "        x[idx] = tmp_val\n",
    "        it.iternext()\n",
    "        \n",
    "    return grad\n",
    "\n",
    "def error_val(x,t):\n",
    "    y=np.dot(x,W)+b\n",
    "    return (np.sum((t-y)**2)/(len(x)))\n",
    "            \n",
    "def predict(x):\n",
    "    y=np.dot(x,W)+b\n",
    "    return y\n",
    "\n",
    "learning_rate=1e-5\n",
    "\n",
    "f = lambda x: loss_func(x_data,t_data)  # f(x) = loass_func(x_data,y_data)\n",
    "\n",
    "print(f\"initial error value = {error_val(x_data,t_data)}, initial W = {W}\")\n",
    "\n",
    "for step in range(10001):\n",
    "    W -= learning_rate*numerical_derivative(f,W)\n",
    "    b -= learning_rate*numerical_derivative(f,b)\n",
    "    \n",
    "    if(step % 400 ==0):\n",
    "        print(f\"step = {step}, error_value = {error_val(x_data,t_data)}, W = {W}, b = {b}\")\n",
    "\n",
    "test_data = np.array([100,98,81])\n",
    "predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
