{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a212f67-c702-4f95-b03c-1eccae06fd88",
   "metadata": {},
   "source": [
    "# 전이학습(Transfer Learning) 개념 및 필요성\n",
    "### 학습 데이터 부족\n",
    "- CNN 모델의 품질을 높이기 위해서는, 즉 임의의 데이터에 대해서 정확도는 높이고 오버피팅은 줄이기 위해서는 기본적으로 많은 양의 데이터를 이용하여 학습해야 함.\n",
    "- 그러나, 많은 학습 데이터를 확보하려면 많은 비용과 시간이 소요되기 때문에 현실적으로 쉽지 않음. 이러한 데이터가 부족한 어려움을 해결하기 위해 등장한 것이 전이학습(Transfer Learning)임.\n",
    "\n",
    "### 전이학습(Transfer Learning) 개념 및 필요성\n",
    "- <b>전이학습(Transfer Learning)</b>이란 아주 큰 dataset,\n",
    "- 즉 21,841 부류에 대해서 총 1천4백만장 이상의 이미지로 구성된 ImageNet 데이터를 사용해서 학습된 모델의 가중치를 가져와서,\n",
    "- 우리가 해결하려는 문제에 맞게 보정해서 사용하는 것을 의미함.\n",
    "- 이때 큰 dataset을 사용해서 훈련된 모델을 <b>사전 학습 모델(pre-trained model)</b>이라고 함.\n",
    "- ImageNet 데이터의 이미지 크기는 평균적으로 469X387 임.\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/25-1.png\" style=\"max-width: 60%; height: auto;\"><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287aa95-68af-4a98-9b6d-e66e96d66d29",
   "metadata": {},
   "source": [
    "# 사전 학습 모델 (pre-trained model)\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/25-2.png\" style=\"max-width: 60%; height: auto;\"><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce0abe-b22f-45b0-be16-bc8d5690f1ed",
   "metadata": {},
   "source": [
    "# Fine-tuning\n",
    "\n",
    "- 사전 학습 모델의 가중치를 미세하게 조정하는 기법이며, 새롭게 분류하려는 데이터의 종류와 전체 개수를 미리 분석한 후에,\n",
    "- 그것을 바탕으로 사전 학습 모델 가중치 일부만을 재학습 시키거나 또는 모든 가중치를 처음부터 다시 학습 시킬 수 있음.\n",
    "- Fine-tuning 진행 시 많은 연산량이 필요하므로 일반적으로 CPU보다는 GPU를 많이 사용함. \n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/25-3.png\" style=\"max-width: 60%; height: auto;\"><p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944d76e7-8598-4f19-97c6-89dda4b2450d",
   "metadata": {},
   "source": [
    "# Transfer Learning 사용법\n",
    "\n",
    "<pre>\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# tensorflow에서 제공하는 ImageNet으로 사전학습된 다양한 사전학습 모델 VGG16, ResNet50, MobileNet, InceptionV3 등\n",
    "    \n",
    "base_model = tf.keras.applications.VGG16(<b>weights='imagenet',</b>  # 사전학습에 사용된 dataset\n",
    "                                         <b>include_top=False,</b>   # False: 사전학습에 사용된 특징추출기만 가져옴. True: 특징추출기완 분류기 모두 가져옴.\n",
    "                                         <b>input_shape=(240,240,3))</b> $ 새롭게 학습 시킬 이미지 텐서 크기\n",
    "base_model.summary() </pre>\n",
    "\n",
    "#### [[tensorflow에서 제공하는 사전학습 모델]](https://keras.io/api/applications/) https://keras.io/api/applications/\n",
    "#### [[top-1, top-5 accuracy]](https://enjoyso.tistory.com/122)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15d0bf03-3dcb-4672-aed8-02284340f683",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-12 22:40:05.375495: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58889256/58889256 [==============================] - 7s 0us/step\n",
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 240, 240, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 240, 240, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 240, 240, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 120, 120, 64)      0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 120, 120, 128)     73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 120, 120, 128)     147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 60, 60, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 60, 60, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 60, 60, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 60, 60, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 30, 30, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 30, 30, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 30, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 30, 30, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 15, 15, 512)       0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 15, 15, 512)       2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "\n",
    "# ImageNet으로 사전학습된 VGG16, ResNet50, MobileNet, InceptionV3 등\n",
    "\n",
    "base_model = tf.keras.applications.VGG16(weights='imagenet',\n",
    "                                         include_top=False,\n",
    "                                         input_shape=(240,240,3))\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfaf0b8f-41cf-41c4-84bd-409ba3f1c2b4",
   "metadata": {},
   "source": [
    "# VGG16 사전학습모델 \n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/25-4.png\" style=\"max-width: 60%; height: auto;\"><p>\n",
    "[[참고자료]](https://89douner.tistory.com/270)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e7d049-977d-405f-9379-d63b7bc630aa",
   "metadata": {},
   "source": [
    "# 새로운 분류기\n",
    "<pre>\n",
    "model = Sequential()\n",
    "\n",
    "model.add(base_model)    # 사전학습 모델의 특징추출기 \n",
    "\n",
    "model.add(Flatten())     #  Flatten() 함수 이외에 GlobalAveragePooling2D()도 많이 사용됨.\n",
    "\n",
    "model.add(Dense(64, activation='relu'))        # 새로운 분류기 \n",
    "model.add(Dropout(0.25))                       #\n",
    "model.add(Dense(4, activation='softmax'))      # New classifier \n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()    \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94d38cf3-9cfb-4ef1-8e7f-03679607be8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 25088)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                1605696   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4)                 260       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,320,644\n",
      "Trainable params: 16,320,644\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(base_model)\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', \n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
