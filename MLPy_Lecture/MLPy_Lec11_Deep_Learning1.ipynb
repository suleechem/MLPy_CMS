{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7353b6b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# XOR @ Single-Logistic Regresstion\n",
    "- 1개의 logistic regression만으로 XOR문제를 해결 할 수 없음.\n",
    "- NAND, OR $\\rightarrow$ AND 여러개의 logistic regression으로 구현 할 수 있음.\n",
    "  \n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-1.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a96db74",
   "metadata": {},
   "source": [
    "# XOR @ Multi-dimensional Concept\n",
    "\n",
    "- 이는 차원의 확장, 다차원개념 적용과 유사함.\n",
    "- 2차원에서 해결 할 수 없는 문제를 3차원으로 차원을 확장함으로써 해결 가능함.\n",
    "  \n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-2.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816c995",
   "metadata": {},
   "source": [
    "# XOR @ Multi-Logistic Regression\n",
    "- <b>\"NAND, OR $\\rightarrow$ AND 여러개의 logistic regression\"</b> 구현은 <b>인간의 뇌에서 \"다차원 개념\"을 생각하는 메커니즘</b>과 유사함.\n",
    "- <b>Single logistic regression $\\rightarrow$ multi-logistic regression</b>\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-3.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70856e3",
   "metadata": {},
   "source": [
    "# Perceptron\n",
    "- 인간의 뇌에서는 <b>\"뉴런(neuron) $\\rightarrow$ 신경망(neural network) $\\rightarrow$ 지능(intelligence)\"</b>의 메커니즘을 따름.\n",
    "- 이는 <b>\"퍼셉트론(perceptron) $\\rightarrow$ 인공신경망(artificial neural network) $\\rightarrow$ 인공지능(artificial intelligence(AI))\"</b>에 대응됨.\n",
    "  \n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-4.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6dfcd",
   "metadata": {},
   "source": [
    "# Activation Function (활성화함수)\n",
    "\n",
    "-  신경 세포 neuron은 이전 neuron으로부터 입력신호를 받아 다른 신호를 생성하여 다음 neuron으로 신호를 전달함.\n",
    "-  그러나 입력에 비례하여 출력을 내는 형태$(y=WX)$가 아니라, <font color=red>\"입력 값들의 모든 합이 어떤 임계점(threshold)\"</font>에 도달해야만 출력 신호를 발생 시킴. \n",
    "- 임계점을 넘어서는 경우 출력을 생성해주는 함수를 <font color=red>\"활성화 함수(activation function)\"</font>라고 함. Logistic regression에서 sigmoid함수가 대표적인 활성화 함수임.\n",
    "- @ Sigmoid $\\ \\ $ $y(z) = \\dfrac{1}{1+e^{-z}}=\\dfrac{1}{1+e^{-(Wx+b)}}$,  $ \\ \\ if \\ y(z)  \\ge 0.5 \\rightarrow \\ y(z) = 1$, $\\ \\ if \\ y(z)  < 0.5 \\rightarrow \\ y(z) = 0$\n",
    "- 하지만, <b>Sigmoid</b>함수는 뒤에서 논의할 <b>back propagation</b> 측면에서 문제가 있음. $\\longrightarrow$ <b>RelLu</b> 함수를 사용함. \n",
    "  \n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-5.png\" style=\"max-width: 70%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d96dac",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (인공신경망)\n",
    "- <b>Artificial Neural Network</b> (인공신경망)은 여러층 (layer)로 구성되며, 각 층(<b>layer</b>)는 여러개의 perceptron (multi-logistic regression (<b>multi-node</b>))으로 구성됨.\n",
    "- ANN은 <b>입력층(input layer),출력층(output layer), 그리고 여러개의 은닉층(hidden layer)</b>로 구성된다.\n",
    "- Hidden layer개수가 증가할 수록 깊이가 깊어진다하여 <b>deep learning</b>이라 칭하게 됨. \n",
    "  \n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-6.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08da2f2b",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "- 데이터가 Input Layer에서 Output Layer로 전파 (Propagation)될때, 각 layer에 있는 노드의 <font color=red><b>모든 가중치 $W_{11}, W_{12}, \\dots W_{MK}$는 출력값의 오차가 최소값</b></font>이 되도록 학습 시킴.\n",
    "- Hidden layer를 깊게(Deep) 할수록 정확도가 높아짐. <font color=red><b> $\\rightarrow$Depp Learning </b></font>\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-7.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ede7ab-4549-4334-b74c-4c2294808ab2",
   "metadata": {},
   "source": [
    "- $\\sigma(z) = \\dfrac{1}{1+e^{-z}}=\\dfrac{1}{1+e^{-(Wx+b)}}$\n",
    "- 모든 가중치 $W \\ and \\ b$는 <b>matrix 혹은 vector</b> 형태를 가지게 됨.\n",
    "- XRO문제의 예시.\n",
    " \n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-9.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d64552",
   "metadata": {},
   "source": [
    "# Feed Forward notation\n",
    "- $input\\ layer \\rightarrow \\ hidden \\ layer \\dots \\rightarrow output \\ layer$ 데이터의 흐름을 <b>feed forward</b>라고 함.\n",
    "- Weight notation\n",
    "> <b>가중치 $W_{21}^{(2)}$</b>: layer2의 노드에 적용되는 가중치로서, layer1의 node1에서 layer2의 node2로 전달되는 신호를 강화 또는 약화시키는 가중치  \n",
    "> - 가중치에서 아래첨자는 다음 layer의 node number가 먼저 나옴\n",
    "- Bias notation\n",
    "  > <b>바이어스 $b_1^{(2)}$</b>: layer2에 있는 node1에 적용되는 바이어스\n",
    "- Linear regression notation\n",
    "  > <b>Linear regression 계산값 $z_2^{(2)}$</b>: layer2의 node2 linear regression 계산값 $(z_2^{(2)}=x_1W_{21}^{(2)} + x_2W_{22}^{(2)} + b_2^{(2)}$)\n",
    "- <b>Node output notation</b>\n",
    "  > <b>node의 출력값 $a_2^{{2}}$</b>: layer2의 node2출력값으로서, logistic regression 계산값.\n",
    "  > 활성화함수(activation function)로서 sigmoid를 사용하면, $a_2^{(2)}=sigmoid(z_2^{(2)})$\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-8.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23f26c-8aaf-44ac-9df4-f7bb0c1f7a1d",
   "metadata": {},
   "source": [
    "- 입력층(input layer)으로 데이터가 입력되고, 1개 이상으로 구성된 hidden layers를 거쳐 마지막으로 출력층(output layer)으로 출력값을 내보내는 과정.\n",
    "- 입력층 출력\n",
    "  > - deep learning의 input layer에서는 activation function을 적용하지 않고, 입력삾 그대로를 출력으로 내보냄.  \n",
    "  > - $(a_1^{(1)} \\ \\ a_2^{(1)} )=(x_1 \\ \\ x_2)$\n",
    "- 은닉층의 선형회귀 값\n",
    "  > - <img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-10.png\" style=\"max-width: 80%; height: auto;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a4a331-8448-48c7-be3f-0ded8b6541db",
   "metadata": {},
   "source": [
    "- 은닉층 출력\n",
    "  > - Activation function이 Sigmoid이므로 출력값은 0~1 사이의 값만을 가짐.   \n",
    "  > - $a_1^{(2)}=sigmoid(z_1^{(2)})$   \n",
    "  > - $a_2^{(2)}=sigmoid(z_2^{(2)})$\n",
    "- 출력층 선형회귀 값\n",
    "  > - <img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-11.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6bfd0e0-700a-42bf-acb7-a874523d1223",
   "metadata": {},
   "source": [
    "- 출력층 출력\n",
    "  > - $y=a_1^{(3)}=sigmoid(z_1^{(3)}$   \n",
    "  > - 출력값 $a_1^{(3)}$은, 입력값 데이터에 대해 최종적으로 계산해야 하는 $y$값 이며, 이러한 <font color=red><b>$y$값과 정답 $t$와의 차이인 오차(loss)가 최소가 되도록 각층의 가중치(weight)와 바이어스(bias)를 최적화</b> </font> 해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2639a4-7ec4-4d8d-8074-58beb3517d3e",
   "metadata": {},
   "source": [
    "# Calculation process of [W, b] @ Deep learning\n",
    "\n",
    "<img src = \"http://cmseng.skku.edu/CMSLecture/ML/img/11-12.png\" style=\"max-width: 80%; height: auto;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
